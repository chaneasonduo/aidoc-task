{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4821a184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-24 15:23:10 [__init__.py:241] Automatically detected platform cuda.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "libtorch_cuda.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01masyncio\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SamplingParams\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marg_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AsyncEngineArgs\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msampling_params\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RequestOutputKind\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masync_llm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AsyncLLM\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/.venv/lib/python3.11/site-packages/vllm/engine/arg_utils.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypeIs, deprecated\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menvs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menvs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (BlockSize, CacheConfig, CacheDType, CompilationConfig,\n\u001b[32m     25\u001b[39m                          ConfigFormat, ConfigType, ConvertOption,\n\u001b[32m     26\u001b[39m                          DecodingConfig, DetailedTraceModules, Device,\n\u001b[32m     27\u001b[39m                          DeviceConfig, DistributedExecutorBackend,\n\u001b[32m     28\u001b[39m                          GuidedDecodingBackend, HfOverrides, KVEventsConfig,\n\u001b[32m     29\u001b[39m                          KVTransferConfig, LoadConfig, LogprobsMode,\n\u001b[32m     30\u001b[39m                          LoRAConfig, MambaDType, ModelConfig, ModelDType,\n\u001b[32m     31\u001b[39m                          ModelImpl, MultiModalConfig, ObservabilityConfig,\n\u001b[32m     32\u001b[39m                          ParallelConfig, PoolerConfig, PrefixCachingHashAlgo,\n\u001b[32m     33\u001b[39m                          RunnerOption, SchedulerConfig, SchedulerPolicy,\n\u001b[32m     34\u001b[39m                          SpeculativeConfig, TaskOption, TokenizerMode,\n\u001b[32m     35\u001b[39m                          VllmConfig, get_attr_docs, get_field)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CpuArchEnum, current_platform\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/.venv/lib/python3.11/site-packages/vllm/config/__init__.py:36\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (BlockSize, CacheConfig, CacheDType, MambaDType,\n\u001b[32m     33\u001b[39m                                PrefixCachingHashAlgo)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompilation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (CompilationConfig, CompilationLevel,\n\u001b[32m     35\u001b[39m                                      CUDAGraphMode, PassConfig)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DistributedExecutorBackend, ParallelConfig\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscheduler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SchedulerConfig, SchedulerPolicy\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConfigType, config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/.venv/lib/python3.11/site-packages/vllm/config/parallel.py:17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m current_platform\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cuda_device_count_stateless, get_open_port\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/.venv/lib/python3.11/site-packages/vllm/platforms/__init__.py:273\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _current_platform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    272\u001b[39m     platform_cls_qualname = resolve_current_platform_cls_qualname()\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     _current_platform = \u001b[43mresolve_obj_by_qualname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mplatform_cls_qualname\u001b[49m\u001b[43m)\u001b[49m()\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mglobal\u001b[39;00m _init_trace\n\u001b[32m    276\u001b[39m     _init_trace = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(traceback.format_stack())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/.venv/lib/python3.11/site-packages/vllm/utils/__init__.py:2568\u001b[39m, in \u001b[36mresolve_obj_by_qualname\u001b[39m\u001b[34m(qualname)\u001b[39m\n\u001b[32m   2564\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2565\u001b[39m \u001b[33;03mResolve an object by its fully-qualified class name.\u001b[39;00m\n\u001b[32m   2566\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2567\u001b[39m module_name, obj_name = qualname.rsplit(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2568\u001b[39m module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, obj_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/.venv/lib/python3.11/site-packages/vllm/platforms/cuda.py:18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParamSpec\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# import custom ops, trigger op registration\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menvs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menvs\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_logger\n",
      "\u001b[31mImportError\u001b[39m: libtorch_cuda.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0\n",
    "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n",
    "\"\"\"\n",
    "Simple example demonstrating streaming offline inference with AsyncLLM (V1 engine).\n",
    "\n",
    "This script shows the core functionality of vLLM's AsyncLLM engine for streaming\n",
    "token-by-token output in offline inference scenarios. It demonstrates DELTA mode\n",
    "streaming where you receive new tokens as they are generated.\n",
    "\n",
    "Usage:\n",
    "    python examples/offline_inference/async_llm_streaming.py\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "\n",
    "from vllm import SamplingParams\n",
    "from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "from vllm.sampling_params import RequestOutputKind\n",
    "from vllm.v1.engine.async_llm import AsyncLLM\n",
    "\n",
    "\n",
    "async def stream_response(engine: AsyncLLM, prompt: str, request_id: str) -> None:\n",
    "    \"\"\"\n",
    "    Stream response from AsyncLLM and display tokens as they arrive.\n",
    "\n",
    "    This function demonstrates the core streaming pattern:\n",
    "    1. Create SamplingParams with DELTA output kind\n",
    "    2. Call engine.generate() and iterate over the async generator\n",
    "    3. Print new tokens as they arrive\n",
    "    4. Handle the finished flag to know when generation is complete\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 Prompt: {prompt!r}\")\n",
    "    print(\"💬 Response: \", end=\"\", flush=True)\n",
    "\n",
    "    # Configure sampling parameters for streaming\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=100,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        seed=42,  # For reproducible results\n",
    "        output_kind=RequestOutputKind.DELTA,  # Get only new tokens each iteration\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Stream tokens from AsyncLLM\n",
    "        async for output in engine.generate(\n",
    "            request_id=request_id, prompt=prompt, sampling_params=sampling_params\n",
    "        ):\n",
    "            # Process each completion in the output\n",
    "            for completion in output.outputs:\n",
    "                # In DELTA mode, we get only new tokens generated since last iteration\n",
    "                new_text = completion.text\n",
    "                if new_text:\n",
    "                    print(new_text, end=\"\", flush=True)\n",
    "\n",
    "            # Check if generation is finished\n",
    "            if output.finished:\n",
    "                print(\"\\n✅ Generation complete!\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during streaming: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "async def main():\n",
    "    print(\"🔧 Initializing AsyncLLM...\")\n",
    "\n",
    "    # Create AsyncLLM engine with simple configuration\n",
    "    engine_args = AsyncEngineArgs(\n",
    "        model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "        enforce_eager=True,  # Faster startup for examples\n",
    "    )\n",
    "    engine = AsyncLLM.from_engine_args(engine_args)\n",
    "\n",
    "    try:\n",
    "        # Example prompts to demonstrate streaming\n",
    "        prompts = [\n",
    "            \"The future of artificial intelligence is\",\n",
    "            \"In a galaxy far, far away\",\n",
    "            \"The key to happiness is\",\n",
    "        ]\n",
    "\n",
    "        print(f\"🎯 Running {len(prompts)} streaming examples...\")\n",
    "\n",
    "        # Process each prompt\n",
    "        for i, prompt in enumerate(prompts, 1):\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(f\"Example {i}/{len(prompts)}\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "\n",
    "            request_id = f\"stream-example-{i}\"\n",
    "            await stream_response(engine, prompt, request_id)\n",
    "\n",
    "            # Brief pause between examples\n",
    "            if i < len(prompts):\n",
    "                await asyncio.sleep(0.5)\n",
    "\n",
    "        print(\"\\n🎉 All streaming examples completed!\")\n",
    "\n",
    "    finally:\n",
    "        # Always clean up the engine\n",
    "        print(\"🔧 Shutting down engine...\")\n",
    "        engine.shutdown()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        asyncio.run(main())\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n🛑 Interrupted by user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59ffc5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vllm                              0.10.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list | grep vllm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
